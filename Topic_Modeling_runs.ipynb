{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\cygwin64\\home\\darsh\\Continuum\\Anaconda3\\lib\\site-packages\\socks.py:58: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Callable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\cygwin64\\home\\darsh\\Continuum\\Anaconda3\\lib\\site-packages\\thinc\\neural\\train.py:7: DeprecationWarning:\n",
      "\n",
      "Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "\n",
      "C:\\cygwin64\\home\\darsh\\Continuum\\Anaconda3\\lib\\site-packages\\thinc\\check.py:4: DeprecationWarning:\n",
      "\n",
      "Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# regular imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# imports for scikit-learn & LDA\n",
    "import sklearn\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "import concurrent.futures\n",
    "\n",
    "# imports for scikit-learn & LDA\n",
    "import pyLDAvis.sklearn\n",
    "from pylab import bone, pcolor, colorbar, plot, show, rcParams, savefig\n",
    "\n",
    "# Plotly based imports for visualization\n",
    "from plotly import tools\n",
    "import plotly.plotly as py\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# spaCy based imports\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Define the punctuations & stop words\n",
    "PUNCTUATIONS = string.punctuation\n",
    "STOPWORDS = list(STOP_WORDS)\n",
    "\n",
    "# Load the spacy model installed (using the medium model)\n",
    "NLP = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the working directory & raw input datasets\n",
    "REL_PATH = './'\n",
    "INFILE = '050319_acled_all.csv'\n",
    "\n",
    "# Define the directory for saving LDA visualizations as HTML files\n",
    "LDA_VIS_PATH = './lda_vis/'\n",
    "\n",
    "# Read in the raw file\n",
    "df = pd.read_csv(os.path.join(REL_PATH, INFILE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parser & Tokenizer function for conlfict notes\n",
    "parser = English()\n",
    "def spacy_tokenizer(note):\n",
    "    mytokens = parser(str(note))\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "    mytokens = [ word for word in mytokens if word not in STOPWORDS and word not in PUNCTUATIONS ]\n",
    "    mytokens = \" \".join([i for i in mytokens])\n",
    "    return mytokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 509157/509157 [10:34<00:00, 802.87it/s] \n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "# Parsing & Tokenizing the entire dataset\n",
    "df[\"processed_notes\"] = df[\"notes\"].progress_apply(spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class definition for LDA Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:16: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\-\n",
      "\n",
      "<>:16: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\-\n",
      "\n",
      "<>:16: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\-\n",
      "\n",
      "<ipython-input-5-82b98bec0574>:16: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\-\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class LDA_Model(object):\n",
    "    \"\"\"docstring for LDA_Model.\"\"\"\n",
    "\n",
    "    def __init__(self, num_topics, max_iter, vectorizer_type):\n",
    "        super(LDA_Model, self).__init__()\n",
    "        self.num_topics = num_topics\n",
    "        self.max_iter = max_iter\n",
    "        self.lda = LatentDirichletAllocation(n_components=self.num_topics,\n",
    "                                             max_iter=self.max_iter,\n",
    "                                             learning_method='online',\n",
    "                                             learning_offset=50.,\n",
    "                                             random_state=0)\n",
    "        self.count_vectorizer = CountVectorizer(min_df=5, max_df=0.9,\n",
    "                                                stop_words=STOPWORDS,\n",
    "                                                lowercase=True,\n",
    "                                                token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}')\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                                stop_words=STOPWORDS)\n",
    "        # Choose vectorizer type based on flag passed in\n",
    "        self.vectorizer_type = vectorizer_type\n",
    "        if self.vectorizer_type == 'tfidf':\n",
    "            self.vectorizer = self.tfidf_vectorizer\n",
    "        else:\n",
    "            self.vectorizer = self.count_vectorizer\n",
    "\n",
    "    def vectorize_data(self, list):\n",
    "        self.data_vectorized = self.vectorizer.fit_transform(tqdm(list))\n",
    "\n",
    "    def train_lda(self):\n",
    "        self.lda.fit(self.data_vectorized)\n",
    "\n",
    "    def selected_topics(self, top_n=10):\n",
    "        for idx, topic in enumerate(self.lda.components_):\n",
    "            print(\"LDA Model:\")\n",
    "            print(\"Topic %d:\" % (idx))\n",
    "            print([(self.vectorizer.get_feature_names()[i], topic[i]) \\\n",
    "                               for i in topic.argsort()[:-top_n - 1:-1]])\n",
    "\n",
    "    def visualize(self, out_vis_file):\n",
    "        # Build a Visualization using pyLDAvis\n",
    "        self.dash = pyLDAvis.sklearn.prepare(self.lda,\n",
    "                                             self.data_vectorized,\n",
    "                                             self.vectorizer,\n",
    "                                             mds='tsne')\n",
    "        # Save the Visualization built as an HTML file\n",
    "        pyLDAvis.save_html(self.dash, fileobj=os.path.join(LDA_VIS_PATH, out_vis_file))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hyperparameters\n",
    "grid = {'word2vec': {'dataset': ['full', '100+'], \n",
    "                     'tokens': ['unigrams', 'bigrams'],\n",
    "                     'num_topics': [3, 5, 7, 10, 15],\n",
    "                     'max_iter': [2, 5, 10],\n",
    "                     'vectorizer_type': ['count', 'tfidf']\n",
    "                    },\n",
    "        'custom':  {'dataset': ['full', '100+'], \n",
    "                     'tokens': ['unigrams', 'bigrams'],\n",
    "                     'num_topics': [3, 5, 7, 10, 15],\n",
    "                     'max_iter': [2, 5, 10],\n",
    "                     'vectorizer_type': ['count', 'tfidf']\n",
    "                    }\n",
    "       }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1: 3 topics, 2 iterations, count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test parameters\n",
    "num_topics = 3\n",
    "max_iter = 2\n",
    "vectorizer_type = 'count'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the LDA Model\n",
    "lda_3_2_count = LDA_Model(num_topics, max_iter, vectorizer_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 509157/509157 [00:18<00:00, 27826.77it/s]\n"
     ]
    }
   ],
   "source": [
    "# Vectorize Data\n",
    "lda_3_2_count.vectorize_data(df[\"processed_notes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Train the model\n",
    "lda_3_2_count.train_lda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 19min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Build & save Visualization as HTML\n",
    "lda_3_2_count.visualize('lda_3_2_count_full_unigram.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2: 3 topics, 5 iterations, count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 509157/509157 [00:17<00:00, 28776.34it/s]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the LDA Model\n",
    "lda_3_5_count = LDA_Model(num_topics=3, max_iter=5, vectorizer_type='count')\n",
    "\n",
    "# Vectorize Data\n",
    "lda_3_5_count.vectorize_data(df[\"processed_notes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 16min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Train the model\n",
    "lda_3_5_count.train_lda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 21min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Build & save Visualization as HTML\n",
    "lda_3_5_count.visualize('lda_3_5_count_full_unigram.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3: 3 topics, 2 iterations, tfidf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 509157/509157 [00:21<00:00, 23794.43it/s]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the LDA Model\n",
    "lda_3_2_tfidf = LDA_Model(num_topics=3, max_iter=2, vectorizer_type='tfidf')\n",
    "\n",
    "# Vectorize Data\n",
    "lda_3_2_tfidf.vectorize_data(df[\"processed_notes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Train the model\n",
    "lda_3_2_tfidf.train_lda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 42min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Build & save Visualization as HTML\n",
    "lda_3_2_tfidf.visualize('lda_3_2_tfidf_full_unigram.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 4: 5 topics, 2 iterations, count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 509157/509157 [00:18<00:00, 27116.46it/s]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the LDA Model\n",
    "lda_5_2_count = LDA_Model(num_topics=5, max_iter=2, vectorizer_type='count')\n",
    "\n",
    "# Vectorize Data\n",
    "lda_5_2_count.vectorize_data(df[\"processed_notes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Train the model\n",
    "lda_5_2_count.train_lda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 18min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Build & save Visualization as HTML\n",
    "lda_5_2_count.visualize('lda_5_2_count_full_unigram.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 5: 5 topics, 2 iterations, tfidf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 509157/509157 [00:12<00:00, 42061.73it/s]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the LDA Model\n",
    "lda_5_2_tfidf = LDA_Model(num_topics=5, max_iter=2, vectorizer_type='tfidf')\n",
    "\n",
    "# Vectorize Data\n",
    "lda_5_2_tfidf.vectorize_data(df[\"processed_notes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Train the model\n",
    "lda_5_2_tfidf.train_lda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 25min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Build & save Visualization as HTML\n",
    "lda_5_2_tfidf.visualize('lda_5_2_tfidf_full_unigram.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 6: 7 topics, 2 iterations, count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 509157/509157 [00:07<00:00, 70571.07it/s] \n"
     ]
    }
   ],
   "source": [
    "# Instantiate the LDA Model\n",
    "lda_7_2_count = LDA_Model(num_topics=7, max_iter=2, vectorizer_type='count')\n",
    "\n",
    "# Vectorize Data\n",
    "lda_7_2_count.vectorize_data(df[\"processed_notes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Train the model\n",
    "lda_7_2_count.train_lda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 13min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Build & save Visualization as HTML\n",
    "lda_7_2_count.visualize('lda_7_2_count_full_unigram.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 7: 7 topics, 2 iterations, tfidf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 509157/509157 [00:08<00:00, 58700.73it/s]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the LDA Model\n",
    "lda_7_2_tfidf = LDA_Model(num_topics=7, max_iter=2, vectorizer_type='tfidf')\n",
    "\n",
    "# Vectorize Data\n",
    "lda_7_2_tfidf.vectorize_data(df[\"processed_notes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Train the model\n",
    "lda_7_2_tfidf.train_lda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 25min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Build & save Visualization as HTML\n",
    "lda_7_2_tfidf.visualize('lda_7_2_tfidf_full_unigram.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 8: 10 topics, 2 iterations, count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 509157/509157 [00:07<00:00, 69185.55it/s]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the LDA Model\n",
    "lda_10_2_count = LDA_Model(num_topics=10, max_iter=2, vectorizer_type='count')\n",
    "\n",
    "# Vectorize Data\n",
    "lda_10_2_count.vectorize_data(df[\"processed_notes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Train the model\n",
    "lda_10_2_count.train_lda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Build & save Visualization as HTML\n",
    "lda_10_2_count.visualize('lda_10_2_count_full_unigram.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 9: 10 topics, 2 iterations, tfidf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 509157/509157 [00:08<00:00, 57884.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the LDA Model\n",
    "lda_10_2_tfidf = LDA_Model(num_topics=10, max_iter=2, vectorizer_type='tfidf')\n",
    "\n",
    "# Vectorize Data\n",
    "lda_10_2_tfidf.vectorize_data(df[\"processed_notes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Train the model\n",
    "lda_10_2_tfidf.train_lda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 26min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Build & save Visualization as HTML\n",
    "lda_10_2_tfidf.visualize('lda_10_2_tfidf_full_unigram.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the LDA Models as pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle_path = './lda_pickles/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_10_2_tfidf_pkl = 'lda_10_2_tfidf.pkl'\n",
    "\n",
    "# Open the file to save as pkl file\n",
    "lda_model_pkl = open(os.path.join(pickle_path, lda_10_2_tfidf_pkl), 'wb')\n",
    "pickle.dump(lda_10_2_tfidf, lda_model_pkl)\n",
    "\n",
    "# Close the pickle instances\n",
    "lda_model_pkl.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded LDA model ::  <__main__.LDA_Model object at 0x00000229ABE32160>\n",
      "Wall time: 386 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Loading the saved model pickle\n",
    "lda_model_pkl = open(os.path.join(pickle_path, lda_10_2_tfidf_pkl), 'rb')\n",
    "lda_10_2_tfidf = pickle.load(lda_model_pkl)\n",
    "print(\"Loaded LDA model :: \", lda_10_2_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.LDA_Model at 0x22a534ae3c8>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_10_2_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define functions to create and load model pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_dict = {lda_3_2_count : 'lda_3_2_count.pkl',\n",
    "               lda_3_5_count : 'lda_3_5_count.pkl',\n",
    "               lda_3_2_tfidf : 'lda_3_2_tfidf.pkl',\n",
    "               lda_5_2_count : 'lda_5_2_count.pkl',\n",
    "               lda_5_2_tfidf : 'lda_5_2_tfidf.pkl',\n",
    "               lda_7_2_count : 'lda_7_2_count.pkl',\n",
    "               lda_7_2_tfidf : 'lda_7_2_tfidf.pkl',\n",
    "               lda_10_2_count: 'lda_10_2_count.pkl',\n",
    "               lda_10_2_tfidf: 'lda_10_2_tfidf.pkl'\n",
    "               }\n",
    "\n",
    "def pickle_model(lda_obj):\n",
    "    \n",
    "    pkl_name = pickle_dict[lda_obj]\n",
    "    \n",
    "    # Open the file to save as pkl file\n",
    "    lda_model_pkl = open(os.path.join(pickle_path, pkl_name), 'wb')\n",
    "    pickle.dump(lda_obj, lda_model_pkl)\n",
    "\n",
    "    # Close the pickle instances\n",
    "    lda_model_pkl.close()\n",
    "\n",
    "def load_model(lda_obj):\n",
    "    \n",
    "    pkl_name = pickle_dict[lda_obj]\n",
    "    \n",
    "    # Loading the saved model pickle\n",
    "    lda_model_pkl = open(os.path.join(pickle_path, pkl_name), 'rb')\n",
    "    lda_obj = pickle.load(lda_model_pkl)\n",
    "    \n",
    "    return lda_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pickle all Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:08<00:00,  1.10it/s]\n"
     ]
    }
   ],
   "source": [
    "for lda_obj in tqdm(pickle_dict.keys()):\n",
    "    pickle_model(lda_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
