{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as tud\n",
    "from collections import Counter, defaultdict\n",
    "import operator\n",
    "import os, math\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the random seeds so the experiments can be replicated exactly\n",
    "seed = 30255\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Data/data_prepr.csv')\n",
    "data = data[['tagged_rel', 'tagged_str']]\n",
    "\n",
    "data.tagged_str = data.tagged_str.str.replace('AGGRESOR', ' AGGRESOR ')\n",
    "data.tagged_str = data.tagged_str.str.replace('VICTIM', ' VICTIM ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data):\n",
    "    # reset index\n",
    "    data_idx = data.reset_index()\n",
    "    \n",
    "    # subsetting in .75 train and even random split the other\n",
    "    dev_test_size = round(len(data_idx)*0.25)\n",
    "    tr_df = data_idx[dev_test_size:]\n",
    "    dev_test_df = data_idx[:dev_test_size]\n",
    "    ran_idx = np.random.choice(dev_test_size, round(dev_test_size/2), replace=False)\n",
    "    dev_df = dev_test_df.iloc[ran_idx]\n",
    "    test_df = dev_test_df.iloc[~ran_idx]\n",
    "    \n",
    "    #shuffling training so it doesn't see similar cases one after the other \n",
    "    tr_df = tr_df.sample(frac=1).reset_index(drop=True)\n",
    "        \n",
    "    # make them lists\n",
    "    train_l = tr_df.values.tolist()\n",
    "    dev_l = dev_df.values.tolist()\n",
    "    test_l = test_df.values.tolist()\n",
    "    all_l = data_idx.values.tolist()\n",
    "\n",
    "    return train_l, dev_l, test_l, all_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list, dev_list, test_list, all_list = split_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mini subset of data for initial test\n",
    "\n",
    "small_train = train_list[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Process text\n",
    "\n",
    "def remove_stopwords(l):\n",
    "    STOP  = ['ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than'] \n",
    "    l_clean = []\n",
    "    for i in STOP: \n",
    "        if i in l:\n",
    "            l_clean.append(i)\n",
    "    return l_clean \n",
    "\n",
    "def word_tokenize(s):\n",
    "    split_l = s.lower().replace('.', '').replace(',', '').replace(';', '').replace(':', '').replace('!', '').replace('?', '').replace('(', '').replace(')', '').split()\n",
    "    #clean_l = remove_stopwords(split_l)\n",
    "    #return clean_l\n",
    "    \n",
    "    return split_l\n",
    "\n",
    "VOCAB_SIZE = 5000\n",
    "class textModel:\n",
    "    def __init__(self, data):\n",
    "        # Vocabulary is a set that stores every word seen in the \n",
    "        # training data\n",
    "        self.vocab_counts = Counter([word for ix, label, content in data \n",
    "                              for word in word_tokenize(content)]\n",
    "                            ).most_common(VOCAB_SIZE-1)\n",
    "        # word to index mapping\n",
    "        self.word_to_idx = {k[0]: v+1 for v, k in \n",
    "                            enumerate(self.vocab_counts)}\n",
    "        # all the unknown words will be mapped to index 0\n",
    "        self.word_to_idx[\"UNK\"] = 0 \n",
    "        self.idx_to_word = {v:k for k, v in self.word_to_idx.items()}\n",
    "\n",
    "        self.vocab = set(self.word_to_idx.keys())\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        \n",
    "        \n",
    "    def train_model(self, data):\n",
    "        '''\n",
    "        Train the model with the provided training data\n",
    "        '''\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def classify(self, data):\n",
    "        '''\n",
    "        Classify the documents with the model\n",
    "        '''\n",
    "        raise \n",
    "        \n",
    "class labels:\n",
    "    def __init__(self, data):\n",
    "        # Vocabulary is a set that stores every word seen in the \n",
    "        # training data\n",
    "        self.label_counts = list(Counter([label for ix, label, content in data]).items())\n",
    "        # word to index mapping\n",
    "        self.label_to_idx = {k[0]: v+1 for v, k in \n",
    "                            enumerate(self.label_counts)}\n",
    "        # all the unknown words will be mapped to index 0\n",
    "        self.idx_to_label = {v:k for k, v in self.label_to_idx.items()}\n",
    "\n",
    "        self.labels = set(self.label_to_idx.keys())\n",
    "        self.labels_size = len(self.labels)\n",
    "\n",
    "        \n",
    "class TextClassificationDataset(tud.Dataset):\n",
    "    '''\n",
    "    PyTorch provides a common dataset interface. \n",
    "    See https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "    The dataset encodes documents into indices. \n",
    "    With the PyTorch dataloader, you can easily get batched data for \n",
    "    training and evaluation. \n",
    "    '''\n",
    "    def __init__(self, data, labd):\n",
    "        \n",
    "        self.data = data\n",
    "        self.vocab_size = VOCAB_SIZE\n",
    "        self.tm = textModel(data)\n",
    "        self.labs = labd\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def wordToTensor(self, word):\n",
    "        tensor = torch.zeros(1, VOCAB_SIZE, dtype=torch.long)\n",
    "        tensor[0][tm.word_to_idx[word]] = 1\n",
    "        return tensor\n",
    "\n",
    "    # Turn text into a <line_length x 1 x n_letters>,\n",
    "    def lineToTensor(self, textstr):\n",
    "        text = word_tokenize(textstr)\n",
    "        to_ix = tm.word_to_idx\n",
    "        tensor = autograd.Variable(torch.LongTensor([to_ix[w] if w in to_ix else to_ix['UNK']for w in text]))\n",
    "\n",
    "#         text = word_tokenize(textstr)\n",
    "#         tensor = torch.zeros(len(text), 1, VOCAB_SIZE, dtype=torch.long)\n",
    "#         for wi, word in enumerate(text):\n",
    "#             if word in self.tm.word_to_idx: \n",
    "#                 tensor[wi][0][self.tm.word_to_idx[word]] = 1\n",
    "#             else: \n",
    "#                 # I add this exception in order to map those words not in vocab\n",
    "#                 tensor[wi][0][self.tm.word_to_idx['UNK']] = 1\n",
    "\n",
    "        return tensor\n",
    "    \n",
    "    def targetToTensor(self, lab): \n",
    "\n",
    "        label_to_ix = self.labs.label_to_idx\n",
    "        l = label_to_ix [lab]\n",
    "#         tensor = torch.zeros(1, len(label_to_ix), dtype=torch.long)\n",
    "#         print(len(label_to_ix))\n",
    "#         tensor[0][l] = 1\n",
    "        target_idx = torch.tensor([[l]])\n",
    "        return target_idx\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):        \n",
    "        datum = self.data[idx]\n",
    "        label = self.targetToTensor(datum[1])\n",
    "        item = self.lineToTensor(datum[2])\n",
    "        \n",
    "        return item, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.autograd as autograd\n",
    "\n",
    "# https://github.com/claravania/lstm-pytorch/blob/master/model.py\n",
    "torch.manual_seed(1)\n",
    "\n",
    "class LSTMRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, output_size):\n",
    "        super(LSTMRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim,batch_first=True, bidirectional=False)\n",
    "        self.hidden2cat = nn.Linear(hidden_dim, output_size)\n",
    "        self.hidden = self.initHidden()\n",
    "        self.optimizer = torch.optim.Adam(self.parameters())\n",
    "\n",
    "        # From HW\n",
    "        self.loss_fn = nn.NLLLoss(size_average=False)\n",
    "        self.num_epochs = 50\n",
    "        self.best_model = {}\n",
    "        \n",
    "    def initHidden(self):\n",
    "        return (autograd.Variable(torch.zeros(1, 1, self.hidden_dim)),\n",
    "                    autograd.Variable(torch.zeros(1, 1, self.hidden_dim)))\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        #print(batch.size(-1))\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        # embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        \n",
    "        # ht is the last hidden state of the sequences\n",
    "        out_space = self.hidden2cat(self.hidden[0][-1])\n",
    "        #out_space = self.hidden2cat(lstm_out)\n",
    "        output = F.log_softmax(out_space)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "\n",
    "    def train_epoch(self, train_data, l):\n",
    "        '''\n",
    "        '''\n",
    "        data = TextClassificationDataset(train_data, l)\n",
    "        data = tud.DataLoader(data, batch_size = 1, shuffle = True)\n",
    "        # Forward pass\n",
    "            \n",
    "        for sent, targets in data: \n",
    "            self.zero_grad() \n",
    "            self.hidden = self.initHidden()\n",
    "            pred = self.forward(sent)\n",
    "            \n",
    "            # Calculate loss with prediction and target labels\n",
    "\n",
    "            loss = self.loss_fn(pred, targets[0][0])\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Apply optimizer step. \n",
    "            self.optimizer.step()\n",
    "            \n",
    "    def classify(self, val_data, l):\n",
    "        '''\n",
    "        This function classifies/predicts documents into their categories. \n",
    "        '''\n",
    "        classif = []\n",
    "        cuml_loss = 0\n",
    "        \n",
    "        data = TextClassificationDataset(val_data, l)\n",
    "        data = tud.DataLoader(data, batch_size = 1, shuffle = True)\n",
    "        with torch.no_grad():\n",
    "            for sent, targets in data: \n",
    "                self.hidden = self.initHidden()\n",
    "                pred = self.forward(sent)\n",
    "\n",
    "                # Calculate loss with prediction and target labels\n",
    "                loss = self.loss_fn(pred, targets[0][0])\n",
    "                cuml_loss += loss\n",
    "\n",
    "                # fingin the max label for accuracy\n",
    "                pred_idx = ((torch.topk(pred, 1)[1])[0]).item()\n",
    "                \n",
    "                pred_rel = l.idx_to_label[pred_idx]\n",
    "\n",
    "                # storing into output\n",
    "                targ_rel = l.idx_to_label[(targets[0][0].item())]\n",
    "                classif.append((targ_rel, sent, pred_rel))\n",
    "\n",
    "        return classif, cuml_loss            \n",
    "                \n",
    "                \n",
    "    def evaluate_classifier_accuracy(self, classif_dat):\n",
    "        '''\n",
    "        This function evaluates the data with the current model. \n",
    "        data contains both documents and labels. \n",
    "        It calls classify() to make predictions, \n",
    "        and compares with the correct labels to return \n",
    "        the model accuracy on \"data\". \n",
    "        '''\n",
    "        num_errors = 0\n",
    "        # compares target to pred and calculates error rate\n",
    "        for target, note, pred in classif_dat:\n",
    "\n",
    "            if pred != target: \n",
    "                num_errors += 1\n",
    "        pred_error = num_errors/len(classif_dat) \n",
    "        \n",
    "        return (1 - pred_error)\n",
    "    \n",
    "    \n",
    "    def train_model(self, train_data, dev_data, l):\n",
    "        \"\"\"\n",
    "        This function processes the entire training set for multiple epochs.\n",
    "        After each training epoch, evaluate your model on the DEV set. \n",
    "        Save the best performing model on the DEV set to best_model\n",
    "        \"\"\"          \n",
    "\n",
    "        best_acc = 0\n",
    "        for i in range(self.num_epochs):\n",
    "            self.train_epoch(train_data, l)\n",
    "            classif, cuml_loss = self.classify(dev_data, l)\n",
    "            acc = self.evaluate_classifier_accuracy(classif)\n",
    "            print('epoch: {}; accuracy: {}; NLLLoss: {}'.format(i, acc, cuml_loss))\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc \n",
    "                self.best_model['model'] = copy.deepcopy(self)\n",
    "                self.best_model['acc'] = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.autograd as autograd\n",
    "\n",
    "# https://github.com/claravania/lstm-pytorch/blob/master/model.py\n",
    "torch.manual_seed(1)\n",
    "\n",
    "class BiDirLSTMRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, output_size):\n",
    "        super(BiDirLSTMRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim,batch_first=True, bidirectional=True)\n",
    "        self.hidden2cat = nn.Linear(hidden_dim, output_size)\n",
    "        self.hidden = self.initHidden()\n",
    "        self.optimizer = torch.optim.Adam(self.parameters())\n",
    "\n",
    "        # From HW\n",
    "        self.loss_fn = nn.NLLLoss(size_average=False)\n",
    "        self.num_epochs = 50\n",
    "        self.best_model = {}\n",
    "        \n",
    "    def initHidden(self):\n",
    "        return (autograd.Variable(torch.randn(2, 1, self.hidden_dim // 2)),\n",
    "            autograd.Variable(torch.randn(2, 1, self.hidden_dim // 2)))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "366"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMB_DIM = 6\n",
    "HID_DIM = 6\n",
    "labs = labels(all_list)\n",
    "tm = textModel(small_train)\n",
    "tm.vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/crismacg/miniconda3/lib/python3.7/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/Users/crismacg/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0; accuracy: 0.061849873488895146; NLLLoss: 13887.16015625\n",
      "epoch: 1; accuracy: 0.07197076187798712; NLLLoss: 13762.19921875\n",
      "epoch: 2; accuracy: 0.08462187236435204; NLLLoss: 13647.7646484375\n"
     ]
    }
   ],
   "source": [
    "# Running 50 epochs of non-bidirectional LSTM \n",
    "model_lstm = LSTMRNN(EMB_DIM, HID_DIM, tm.vocab_size, labs.labels_size)\n",
    "\n",
    "model_lstm.train_model(small_train, dev_list, labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running 50 epochs of non-bidirectional LSTM \n",
    "pass \n",
    "# ----- code for tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "super(type, obj): obj must be an instance or subtype of type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-b2acc52bb962>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Running 50 epochs of bidirectional LSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_bd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBiDirLSTMRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEMB_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHID_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel_bd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-80-3570d56bc4a0>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, embedding_dim, hidden_dim, vocab_size, output_size)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTMRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: super(type, obj): obj must be an instance or subtype of type"
     ]
    }
   ],
   "source": [
    "# Running 50 epochs of bidirectional LSTM \n",
    "model_bd = BiDirLSTMRNN(EMB_DIM, HID_DIM, tm.vocab_size, labs.labels_size)\n",
    "\n",
    "\n",
    "model_bd.train_model(train_list, dev_list, labs)\n",
    "# ----- code for tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ evaluate both's best model on test\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_ix = labs.label_to_idx\n",
    "tensor = torch.zeros(1, len(label_to_ix), dtype=torch.long)\n",
    "tensor[0][12] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 59])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "35+8+8+8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
